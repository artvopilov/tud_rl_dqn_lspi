"""
Submission template for Programming Challenge 2: Approximate Value Based Methods.


Fill in submission info and implement 4 functions:

- load_dqn_policy
- train_dqn_policy
- load_lspi_policy
- train_lspi_policy

Keep Monitor files generated by Gym while learning within your submission.
Example project structure:

challenge2_submission/
  - challenge2_submission.py
  - dqn.py
  - lspi.py
  - dqn_eval/
  - dqn_train/
  - lspi_eval/
  - lspi_train/
  - supplementary/

Directories `dqn_eval/`, `dqn_train/`, etc. are autogenerated by Gym (see below).
Put all additional results into the `supplementary` directory.

Performance of the policies returned by `load_xxx_policy` functions
will be evaluated and used to determine the winner of the challenge.
Learning progress and learning algorithms will be checked to confirm
correctness and fairness of implementation. Supplementary material
will be manually analyzed to identify outstanding submissions.
"""

import numpy as np
from agent_petya import AgentPetya
from agent_lspi import LSPIAgent
from dqn import dqn_run
from lspi import lspi_run
import random
import torch


random.seed(1)
np.random.seed(1)
torch.manual_seed(1)
torch.cuda.manual_seed(1)


info = dict(
    group_number=None,
    authors="Artem Vopilov; Tomas Pinto",
    description="For DQN: AgentPetya is an agent which learns policy. "
    			"It uses ModelVasya, which is a multilayer neural network. "
    			"AgentPetya has a method act where it exploits ModelVasya to make "
    			"prediction of values of actions and chooses an action with the highest value. "
    			"It also has a method mempry_replay for learning policy. "
    			"There it takes random actions from its memory (storage of acts, rewards, states) "
    			"and trains ModelVasya on its experience. "
    			"The function dqn_run is responsible for connecting the agent with environment. "
    			"It calls methods of the agent to make it memorize and learn."
			    "For LSPI: it uses samples from random actions to learn the policy"
			    "We used polynominal features to approximate the value function",

    additionalMessage="For DQN: pretrainded model model_2hid_tt_adam_a6_cont.pb performs better "
    			"when the length of episode is rather small"
			"For LSPI: Although the LSPI algorithm is able to find a near-optimal policy"
			"during iterations the policy doesn't converge and during training it updates"
			"to intermediate bad policies as it can be shown in Training_LSPI.jpg")


def load_dqn_policy():
    """
    Load pretrained DQN policy from file.

    The policy must return a continuous action `a`
    that can be directly passed to `CartpoleSwingShort-v0` env.

    :return: function pi: s -> a
    """
    agent_for_evaluation = AgentPetya(model_eval=True, model_path='model_3hid_ttt_adam_a6.pb', load=True, epsilon=0)


    return lambda obs: agent_for_evaluation.act(obs)


def train_dqn_policy(env):
    """
    Execute your implementation of the DQN learning algorithm.

    This function should start your code placed in a separate file.

    :param env: gym.Env
    :return: function pi: s -> a
    """
    agent = AgentPetya(epsilon=1, checkpoint_t=1.5, model_path='', gamma=0.995, load=False)
    dqn_run(env, agent)
    agent.evaluate()

    return lambda obs: agent.act(obs)


def load_lspi_policy():
    """
    Load pretrained LSPI policy from file.

    The policy must return a continuous action `a`
    that can be directly passed to `CartpoleStabShort-v0` env.

    :return: function pi: s -> a
    """
    agent = LSPIAgent(epsilon=0.0,
                  discount=0.995,
                  possible_actions=[-20,-12,-6,-3,-2,-1,0,1,2,3,6,12,20],
                  model_path="best_w_LSPI.npy")


    return lambda obs: agent.play(obs)


def train_lspi_policy(env):
    """
    Execute your implementation of the LSPI learning algorithm.

    This function should start your code placed in a separate file.

    :param env: gym.Env
    :return: function pi: s -> a
    """
    agent = LSPIAgent(epsilon=0.0,
                discount=0.995,
                possible_actions=[-20,-12,-6,-3,-2,-1,0,1,2,3,6,12,20])

    lspi_run(env, agent)
    agent.evaluate()

    return lambda obs: agent.play(obs)


# ==== Example evaluation
def main():
    import gym
    from gym.wrappers.monitor import Monitor
    import quanser_robots

    def evaluate(env, policy, num_evlas=25):
        ep_returns = []
        for eval_num in range(num_evlas):
            episode_return = 0
            dones = False
            obs = env.reset()
            while not dones:
                action = policy(obs)
                obs, rewards, dones, info = env.step(action)
                episode_return += rewards
            ep_returns.append(episode_return)
        return ep_returns

    def render(env, policy):
        obs = env.reset()
        done = False
        while not done:
            env.render()
            act = policy(obs)
            obs, _, done, _ = env.step(act)

    def check(env, policy):
        render(env, policy)
        ret_all = evaluate(env, policy)
        print(np.mean(ret_all), np.std(ret_all))
        env.close()

    # DQN I: Check learned policy
    # env = Monitor(gym.make('CartpoleSwingShort-v0'), 'dqn_eval')
    # policy = load_dqn_policy()
    # check(env, policy)

    # DQN II: Check learning procedure
    # env = Monitor(gym.make('CartpoleSwingShort-v0'), 'dqn_train', video_callable=False)
    # policy = train_dqn_policy(env)
    # check(env, policy)

    # LSPI I: Check learned policy
    # env = Monitor(gym.make('CartpoleStabShort-v0'), 'lspi_eval')
    # policy = load_lspi_policy()
    # check(env, policy)

    # LSPI II: Check learning procedure
    env = Monitor(gym.make('CartpoleStabShort-v0'), 'lspi_train', video_callable=False)
    policy = train_lspi_policy(env)
    check(env, policy)


if __name__ == '__main__':
    main()
